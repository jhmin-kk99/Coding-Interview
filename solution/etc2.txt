Memory Hierarchy란?

컴퓨터 하드웨어의 역사적 발전에 관한 보고서를 작성하고 있는 학생이라고 가정하자. 서가에서 많은 책들을 
뽑아 와서 도서관의 책상에 앉아 자료 조사를 하는 중이다. 그 책들에서 당신이 쓰려고 하는 여러 중요한 컴
퓨터에 대한 내용을 찾았지만, EDSAC에 대한 내용은 없다는 것을 알았다. 당신은 서가로 다시 가서 다른 책
을 더 찾아야 한다. EDSAC에 대한 내용이 담겨 있는 초창기 영국 컴퓨터에 대한 책을 찾게 된다. 일단 책상
위에 관련된 많은 책들을 모아 놓으면, 그 안에서 당신이 필요로 하는 주제를 찾게 될 확률이 매우 높아지게
된다. 이렇게 되면 서가에 다시 갈 필요 없이 단지 책상 위의 책들만을 사용하여 많은 작업을 할 수 있게 될
것이다. 책상 위에 여러 권의 책을 놓아두면, 책상에 책을 한 권만 올려놓고 다른 책을 찾을 때마다 가져다
두고 새 책을 찾아오는 것과 비교할 때 많은 시간을 절약할 수 있게 된다.
도서관에 있는 책을 찾아볼 확률이 모두 똑같지 않듯이, 프로그램이 코드와 데이터를 접근할 확률이 모두 같지는
않다. 반면에 도서관에 있는 모든 책을 책상 위에 올려놓고는 우리가 필요로 하는 것을 빠르게 찾을 수 없듯이
많은 정보량을 갖는 큰 메모리에서 메모리 접근을 빠르게 하는 것은 불가능하다. 도서관에서 책을 찾는 방법과
프로그램이 동작하는 방법에는 이러한 지역성의 원칙(principle of locality)이 똑같이 적용된다.

The Memory Hierarchy Goal
Fact : Large memories are slow and fast memories are small
How do we create a memory that gives the illusion of being large, cheap and fast(most of the time)?
- with hierarchy
- with parallelism

The Principle of Locality
Program access a relatively small portion of the address space at any instant of time
A principle that makes having a memory hierarchy a good idea

If an item is referenced,
# Temporal locality(locality in time) : it will tend to be referenced again soon
-> keep most recently accessed data items closer to the processor
# Spatial locality : nearby items will tend to be referenced soon
-> move blocks consisting of contiguous(연속적) words closer to the processor

메모리 계층 구조는 서로 다른 속도와 크기를 갖는 여러 계층의 메모리로 구성되어 있다. 가장 빠른 메모리
는 더 느린 메모리보다 비트당 가격이 비싸기 때문에 대개 그 크기가 작다. 메모리 계층구조의 목적은 사용자
에게 가장 빠른 메모리가 갖고 있는 접근속도를 제공하면서 동시에 가장 싼 기술로 최대한 큰 메모리 용량을
제공하는 것이다. 프로세서에 가까운 계층은 먼 계층의 부분집합이고, 가장 낮은 계층에는 모든 데이터가 다
저장되어 있다. 캐시의 주요 목적은 더 느린 기본 스토리지 계층에 액세스해야 하는 필요를 줄임으로써 데이터 검색 성능을 높이는 것

# Terminology
Block(or line) : the minimum unit of information that is present(or not) in a cache
Hit Rate : the fraction of memory accesses found in a level of the memory hierarchy
Hit Time : Time to access that level which consists of 
Time to access the block + Time to determine hit/miss
Miss Rate : the fraction of memory accesses not found in a level of the memory hierarchy(1- (Hit Rate))
Miss Penalty : Time to replace a block in that level with the corresponding block from a lower
level which consists of
Time to access the block in the lower level + Time to transmit that block to the level that experienced
the miss + Time to insert the blok in that level + Time to pass the block to the requestor
Hit Time << Miss Penalty

How is the Hierarchy Managed?
registers and memory (by compiler (programmer))
cache and main memory (by the cache controller hardware)
main memory and disks
- by the operating system (virtual memory)
- virtual to physical address mapping assisted by the hardware(TLB)
- by the programmer (files)

# cache mapping 

1. Direct mapping(직접 사상)
메모리 주소에 기반을 두고 할당하는 것

Each memory block is mapped to exactly one block in the cache
- lots of lower level blocks must share blocks in the cache
Address mapping 
(Block address) modulo (# of blocks in the cache)
Have a tag associated with each cache block that contains the address information(the upper portion
of the address) required to identify the block

A direct-mapped cache with 64 blocks, 16 bytes/block
(Assuming 32-bit address)
To what block number does address 1200 map?
- Block address = 1200/16 = 75
- Block number = 75 modulo 64 = 11

Associative Caches
- In a direct mapped cache a memory block maps to exactly one cache block
- Allow more flexible block placement

2. Fully associative cache (완전 연관)
- Allow given block to go in any cache entry
- Requires all entries to be searched at once
- Comparator per entry(expensive)

3. n-way set associative cache (집합 연관)
- Each set contains n entries
- Block number determines which set
(Block number) modulo (#Sets in cache)
- Search all entries in a given set at once
- n comparators(less expensive)

# Replacement Policy
Direct mapped : no choice
Set associative : 
- Prefer non-valid entry, if there is one
- Otherwhise, choose among entries in the set
Least-recently used(LRU) :
- the block replaced is the one that has been unused for the longest time
- Simple for 2-way, manageable for 4-way, too hard beyond that
->
1. Must have hardware to keep track of when each way's block was used relative to the other
blocks in the set
2. For 2-way set associative, takes one bit per set(set the bit when a block is referenced
and reset the other way's bit)
Random :
- Gives approximately the same performance as LRU for high associativity

Benefits of Set Associative Caches
- The choice of direct mapped or set associative depends on the cost of a miss versus the cost of
implementation
- Largest gains are in going from direct mapped to 2-way(20% + reduction in miss rate)

# Block size Considerations
Larger blocks should reduce miss rate
- Due to spatial locality
But in a fixed-sized cache
- Larger blocks -> fewer of them (block개수 줄어듦)
(More competition , increased miss rate)
- Larger blocks -> pollution
Larger miss penalty
- Can override benefit of reduced miss rate
- Early restart and critical-word-first can help

# Handling Cache Hits
Read Hits(l$ and D$)
- this is what we want
Write hits(D$ only)
- require the the cache and memory to be consistent
(always write the date into both the cache block and the next level in the memory hierarchy
(write-through))
(writes run at the speed of the next level in the memory hierarchy (so slow) or can use a write
buffer and stall only if the write buffer is full)
- allow cache and memory to be inconsistent
(write the data only into the cache block (write-back the cache block to the next level in the 
memory hierarchy when that cache block is "evicted"(쫓겨남)))
(need a dirty bit for each data cache block to tell if it needs to be written back to memory
when it is evicted (can use a write buffer to help "buffer" write) backs of dirty blocks)

# Handling Cache Misses(데이터가 캐시에 없어서 충족시킬 수 없는 캐시 데이터 요청)

control unit은 실패를 탐지해야 하며, 메모리(또는 하위 수준의 캐시)로부터 데이터를 가져와서 실패를 처리해야 한다. 
만약 캐시 내에 원하는 데이터가 존재하면 컴퓨터는 아무 일도 없는 것처럼 데이터를 사용할 수 있다. 
cache hit을 처리할 수 있게 processor의 control unit을 수정하는 것은 쉬운 일이지만, 
실패의 경우 몇가지 조작이 더 필요하다. handling cache misses는 processor의 control unit과 별도
제어기의 공동 작업으로 처리된다. 

1. Handling Cache Misses(Single Word Block)
Read misses(l$ and D$)
- stall the pipeline, fetch the block from the next level in the memory hierarchy, install it
in the cache and send the requested word to the processor, then let the pipeline resume
Write misses(D$ only)
- stall the pipeline, fetch the block from the next level in the memory hierarchy, install it
in the cache( which may involve having to evict a dirty block if using a write-back cache),
write the word from the processor to the cache, then let the pipeline resume
- Write allocate - just write the word into the cache updating both the tag and data, no need
to check for cache hit, no need to stall
- No-write allocate - skip the cache write (but must invalidate that cache block since it will
 now hold stale data) and just write the word to the write buffer(and eventually to the next
 memory level), no need to stall if the write buffer isn't full

2. Multiword Block Considerations
Read misses(l$ and D$)
Processed the same as for single word blocks - a miss returns the entire block from memory
Miss penalty grows as block size grows
- Early restart - processor resumes execution as soon as the requested word of the block is
returned
- Requested word first - requested word is transferred from the memory to the cache(and 
processor) first
Nonblocking cache - allows the processor to continue to access the cache while the cache is
handling an earlier miss

Write misses(D$)
If using write allocate must first fetch the block from memory and then write the word to the
block(e.g. for 4 word blocks, a new tag, one word of data from the new block, and three words
of data from the old block)

# Average Access Time
Hit time is also important for performance
- A larger cache will have a longer access time. An increase in hit time will likely add 
another stage to the pipeline.
- At some point the increase in hit time for a larger cache will nullify(무효화) the 
improvement in hit rate leading to a decrease in performance.

Average memory access time (AMAT)
- AMAT = Hit time + Miss rate * Miss penalty

Example
cpu with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, l-cache miss rate = 5%
AMAT = 1 + 0.05 * 20 = 2ns (2 cycles per instruction)

다단계 캐시를 이용한 실패 손실 줄이기
현대의 모든 컴퓨터는 캐시를 사용한다. 프로세서의 빠른 클럭속도와 상대적으로 점점 느려지는 DRAM
접근시간의 차이를 더욱 줄이기 위해, 고성능 마이크로프로세서들은 캐시를 한 계층 더 사용한다. 이 
2차 캐시(secondary cache)는 주로 마이크로프로세서에 내장되어 있으며, 1차 캐시(primary cache)에서
실패가 발생할 때 접근된다. 2차 캐시가 원하는 데이터를 갖고 있으면 실패 손실은 2차 캐시의 접근 
시간이 되며, 이 값은 메인 메모리의 접근 시간보다 훨씬 작다. 1차 캐시와 2차 캐시 둘 다 데이터를 갖
고 있지 않은 경우에는 메인 메모리 접근이 필요하게 되고 더 큰 실패 손실이 발생하게 된다.

# Multilevel Caches
Primary cache attached to CPU
- small, but fast
Level-2 cache services misses from primary cache
- Larger, slower, but still faster than main memory
Main memory services L-2 cache misses
Some high-end systems include L-3 cache

Multilevel Cache Example
Given
- CPU base CPI = 1, clock rate = 4GHz
- Miss rate/instruction = 2%
- Main memory access time = 100ns
With just primary cache
- Miss penalty = 100ns/0.25ns = 400cycles
- Effective CPI = 1 + 0.02 * 400 = 9
Now add L-2 cache
- Access time = 5ns
- Global miss rate to main memory = 0.5%
New Performance with L-2 cache
- Primary miss with L-2 hit
(Penalty = 5ns/0.25ns = 20cycles)
- Primary miss with L-2 miss
(Extra penalty = 400cycles)
- CPI = 1 + 0.02 * 20 + 0.005 * 400 = 3.4
- Performance ratio = 9/3.4 = 2.6

# Multilevel Cache Considerations
Primary cache
- Focus on minimal hit time in support of a shorter clock cycle
(Smaller with smaller block sizes)
L-2 cache
- Focus on low miss rate to avoid main memory access to reduce the penalty of long
main memory access times
(Larger block sizes -> spatial locality, Higher levels of associativity -> low miss rate)
- Hit time has less overall impact
Results
- L-1 cache usually smaller than a single cache
- L-1 block size smaller than L-2 block size

The L1 cache is usually separated into separate Data and Instruction caches
The L2 and higher caches are normally unified caches, i.e. both instructions and data are
in the same cache

# Virtual Memory
Use main memory as a "cache" for secondary memory
- Allows efficient and safe sharing of memory among multiple programs
- Provides the ability to easily run programs larger than the size of physical memory
- Simplifies loading a program for execution by providing for code relocation(재배치)
(i.e. the code can be loaded anywhere in main memory) 
What makes it work? (the Principle of Locality)
- A program is likely to access a relatively small portion of its address space during any period of time
Each program is compiled into its own address space - a "virtual" address space
- During run-time each virtual address must be translated to a physical address(an address in main memory)

A program's address space is divided into pages(all one fixed size) or segments(variable sizes)
- The starting location of each page(either in main memory or in secondary memory) is contained
in the program's page table

A virtual address is translated to a physical address by a combination of hardware and software
So each memory request first requires an address translation from the virtual space to the 
physical space
- A virtual memory miss(i.e. when the page is not in physical memory) is called a page fault

# Page Fault Penalty
Page faults : the data is not in memory, retrieve it from disk
- huge miss penalty, thus pages should be fairly large (e.g. 4KB)
- reducing page faults is important
- can handle the faults in software instead of hardware
- using write-through is too expensive so we use write-back
Try to minimize page fault rate
- Fully associative placement
- smart replacement algorithms

# Page Tables
Stores placement information
- Array of page table entries(PTEs), indexed by virtual page number
- Page table register in CPU points to page table in physical memory
If page is present in memory (valid bit이 1)
- PTE stores the physical page number
- Plus other status bits (referenced, dirty, ...)
If page is not present (valid bit이 0)
- PTE can refer to location in swap space on disk
Page table : 가상 메모리 시스템에서 가상 주소를 실제 주소로 변환해 주는 테이블.
테이블은 메모리에 저장되고 가상 페이지 번호로 인덱스된다. 페이지가 메모리에 있을 때
테이블 엔트리에는 가상 페이지를 담고 있는 실제 페이지 번호가 있다. 이 페이지 테이블은
메모리에 적재되지 않은 페이지의 엔트리를 포함할 수 있다. 메모리 내 페이지 테이블의 위치
를 나타내기 위해 페이지 테이블의 시작 주소를 나타내는 레지스터가 하드웨어에 포함되어 있다.
이 레지스터를 page table register라고 부른다.

가상 페이지의 유효비트가 0이면 page fault가 발생하며, 운영체제가 제어를 넘겨 받게 된다.
운영체제가 제어를 갖게 되면 계층의 다음 수준(대개 자기 디스크)에서 그 페이지를 찾아야 하며,
요구된페이지를 메인 메모리의 어디에 배치시킬 것인지 결정해야 한다.
가상 주소만으로는 디스크 내에 그 페이지가 어디에 있는지를 바로 알 수 없다. 가상 메모리 시스템에서
는 가상 주소공간 내 각 페이지의 디스크상 위치를 알고 있어야 한다. 메모리 내의 페이지가 언제 쫓겨
날지를 미리 알 수 없기 때문에, 운영체제는 대개 프로세스를 생성시킬 때 프로세스의 모든 페이지를 위
한 공간을 플래시 메모리나 디스크 상에 마련한다. 이 디스크 상의 공간을 swap space라고 부른다.

# Replacement and Writes
To reduce page fault rate, prefer least-recently used (LRU) replacement
- Reference bit(aka use bit) in PTE set to 1 on access to page
- Periodically cleared to 0 by OS (주기적으로)
- A page with reference bit = 0 has not been used recently
Disk writes take millions of cycles
- Block at once, not individual locations
- Write through is impractical
- Use write-back
- Dirty bit in PTE set when page is written

# Fast Translation Using a TLB
Address translation would appear to require extra memory references
- one to access the PTE
- Then the actual memory access
But access to page tables has good locality
- So use a fast cache of PTEs within the CPU
- Called a Translation Look-aside Buffer(TLB)(a small cache that keeps track of recently
used address mappings to avoid having to do a page table lookup)
- Typical : 16-512 PTEs, 0.5-1 cycle for hit, 10-100 cycles for miss, 0.01%-1% miss rate
- Misses could be handled by hardware or software

# TLB
Just like any other cache, the TLB can be organized as fully associative, set associative,
or direct mapped
TLB access time is typically smaller than cache access time
(because TLBs are much smaller than caches)
- TLBs are typically not more than 512 entries even on high end machines

# TLB Misses
If page is in memory
- Load the PTE from memory and retry
- Could be handled in hardware
(can get complex for more complicated page table structures)
- Or in software
(raise a special exception, with optimized handler)
- Takes 10's~100's of cycles to find and load the translation info into the TLB
If page is not in memory
- OS handles fetching the page and updating the page table
- Then restart the faulting instruction
- Takes 1,000,000's of cycles to service a page fault

# TLB Miss Handler
TLB miss indicates
- Page present, but PTE not in TLB
- Page not present
Must recognize TLB miss before destination register overwritten
- Raise exception
Handler copies PTE from memory to TLB
- Then restarts instruction
- If page not present, page fault will occur

# Page Fault Handler
Use faulting virtual address to find PTE
Locate page on disk
Choose page to replace
- If dirty, write to disk first
Read page into memory and update page table
Make process runnable again
- Restart from faulting instruction

TLB    /    Page Table    /    Cache    /    Possible? Under what circumstances?
Hit    /    Hit           /    Hit      /    Yes - what we want
Hit    /    Hit           /    Miss     /    Yes - data not in cache, but in memory, 
                                                   although PTE and TLB hit
Miss   /    Hit           /    Hit      /    Yes - TLB miss, PA in page table
Miss   /    Hit           /    Miss     /    Yes - TLB miss, PA in page table, but data 
                                                   not in cache
Miss   /    Miss          /    Miss     /    Yes - page fault
Hit    /    Miss          /    Miss,Hit /    Impossible - TLB translation not possible
                                                          if page is not present in memory
Miss   /    Miss          /    Hit      /    Impossible - data not allowed in cache if
                                                          page is not in memory

# Reducing Translation Time
Can overlap the cache access with the TLB access
- Works when the high order bits of the VA are used to access the TLB while the low order
bits are used as index into cache 
(상위 비트로 TLB hit했을 때,PA를 얻을 수 있다. 이때의 PA Tag를 통해 Cache의 tag와 매칭하고
하위 비트를 통해 Cache의 index와 매칭하여 Cache Hit을 판단한다.)

# Sources of Misses (3C's models)
1. Compulsory misses(aka cold start misses)
- First access to a block (처음 access할 때)
2. Capacity misses
- Due to finite cache size
- A replaced block is later accessed again
3. Conflict misses(aka collision misses) (4의 배수, 8의 배수, ... 배수로 접근하면 똑같은 위
치를 계속 접근할 수도 있음)
- In a direct-mapped or set-associative cache
- Due to competition for entries in a set
- Would not occur in a fully associative cache of the same total size

# Cache Design Trade-offs
Increase cache size
- Decrease capacity misses (Possitive)
- May increase access time, compulsory misses (Negative)
Increase associativity
- Decrease conflict misses
- May increase access time
Increase block size
- Decrease compulsory misses(블럭 수 줄어듦)
- Increases miss penalty. For very large block size, may increase miss rate

# 4 Questions for the Memory Hierarchy
Q1. Where can a entry be placed in the upper level?
Q2. How is a entry found if it is in the upper level
Q3. Which entry should be replaced on a miss?
Q4. What happens on a write?

Q1 & Q2 : Where can a entry be placed/found?

                         # of sets                   /           Entries per set
--------------------------------------------------------------------------------------
Direct mapped            # of entries                /           1

Set associative         (# of entries)/associativity /           Associativity     

Fully associative        1                           /           # of entries

---------------------------------------------------------------------------------------
                         Location method             /           # of comparisons
---------------------------------------------------------------------------------------
Direct mapped            Index                       /           1

Set associative          Index the set;              /           Degree of associativity(n)     
                         compare set's tags
                         
Fully associative        Compare all entries' tags   /           # of entries
                         Separate lookup(page)table              0   

Q3 : Which entry should be replaced on a miss?
Easy for direct mapped - only one choice
Set associative or fully associative
- Random
- LRU (Least Recently Used)
For a 2-way set associative, random replacement has a miss rate about 1.1 times higher
than LRU
LRU is too costly to implement for high levels of associativity (> 4-way) since tracking
the usage information is costly

Q4 : What happens on a write?
Write-through - The information is written to the entry in the current memory level and to
the entry in the next level of the memory hierarchy
- Always combined with write buffer so write waits to next level memory can be eliminated
(as long as the write buffer doesn't fill)
->
read misses don't result in writes(so are simpler and cheaper), easier to implement

Write-back - The information is written only to the entry in the current memory level.
The modified entry is written to next level of memory only when it is replaced.
- Need a dirty bit to keep track of whether the entry is clean or dirty
- Virtual memory systems always use write-back of dirty pages to disk
->
writes run at the speed of the cache; repeated writes require only one write to lower level

