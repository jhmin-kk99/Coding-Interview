Memory Hierarchy란?

컴퓨터 하드웨어의 역사적 발전에 관한 보고서를 작성하고 있는 학생이라고 가정하자. 서가에서 많은 책들을 
뽑아 와서 도서관의 책상에 앉아 자료 조사를 하는 중이다. 그 책들에서 당신이 쓰려고 하는 여러 중요한 컴
퓨터에 대한 내용을 찾았지만, EDSAC에 대한 내용은 없다는 것을 알았다. 당신은 서가로 다시 가서 다른 책
을 더 찾아야 한다. EDSAC에 대한 내용이 담겨 있는 초창기 영국 컴퓨터에 대한 책을 찾게 된다. 일단 책상
위에 관련된 많은 책들을 모아 놓으면, 그 안에서 당신이 필요로 하는 주제를 찾게 될 확률이 매우 높아지게
된다. 이렇게 되면 서가에 다시 갈 필요 없이 단지 책상 위의 책들만을 사용하여 많은 작업을 할 수 있게 될
것이다. 책상 위에 여러 권의 책을 놓아두면, 책상에 책을 한 권만 올려놓고 다른 책을 찾을 때마다 가져다
두고 새 책을 찾아오는 것과 비교할 때 많으 ㄴ시간을 절약할 수 있게 된다.
도서관에 있는 책을 찾아볼 확률이 모두 똑같지 않듯이, 프로그램이 코드와 데이터를 접근할 확률이 모두 같지는
않다. 반면에 도서관에 있는 모든 책을 책상 위에 올려놓고는 우리가 필요로 하는 것을 빠르게 찾을 수 없듯이
많은 정보량을 갖는 큰 메모리에서 메모리 접근을 빠르게 하는 것은 불가능하다. 도서관에서 책을 찾는 방법과
프로그램이 동작하는 방법에는 이러한 지역성의 원칙(principle of locality)이 똑같이 적용된다.

The Momory Hierarchy Goal
Fact : Large memories are slow and fast memories are small
How do we create a memory that gives the illusion of being large, cheap and fast(most of the time)?
- with hierarchy
- with parallelism

The Principle of Locality
Program access a relatively small portion of the address space at any instant of time
A principle that makes having a memory hierarchy a good idea

If an item is referenced,
# Temporal locality(locality in time) : it will tend to be referenced again soon
-> keep most recently accessed data items closer to the processor
# Spatial locality : nearby items will tend to be referenced soon
-> move blocks consisting of contiguous(연속적) words closer to the processor

메모리 계층 구조는 서로 다른 속도와 크기를 갖는 여러 계층의 메모리로 구성되어 있다. 가장 빠른 메모리
는 더 느린 메모리보다 비트당 가격이 비싸기 때문에 대개 그 크기가 작다. 메모리 계층구조의 목적은 사용자
에게 가장 빠른 메모리가 갖고 있는 접근속도를 제공하면서 동시에 가장 싼 기술로 최대한 큰 메모리 용량을
제공하는 것이다. 프로세서에 가까운 계층은 먼 계층의 부분집합이고, 가장 낮은 계층에는 모든 데이터가 다
저장되어 있다. 캐시의 주요 목적은 더 느린 기본 스토리지 계층에 액세스해야 하는 필요를 줄임으로써 데이터 검색 성능을 높이는 것

# Terminology
Block(or line) : the minimum unit of information that is present(or not) in a cache
Hit Rate : the fraction of memory accesses found in a level of the memory hierarchy
Hit Time : Time to access that level which consists of 
Time to access the block + Time to determine hit/miss
Miss Rate : the fraction of memory accesses not found in a level of the memory hierarchy(1- (Hit Rate))
Miss Penalty : Time to replace a block in that level with the corresponding block from a lower
level which consists of
Time to access the block in the lower level + Time to transmit that block to the level that experienced
the miss + Time to insert the blok in that level + Time to pass the block to the requestor
Hit Time << Miss Penalty

How is the Hierarchy Managed?
registers and memory (by compiler (programmer))
cache and main memory (by the cache controller hardware)
main memory and disks
- by the operating system (virtual memory)
- virtual to physical address mapping assisted by the hardware(TLB)
- by the programmer (files)

# cache mapping 

1. Direct mapping(직접 사상)
메모리 주소에 기반을 두고 할당하는 것

Each memory block is mapped to exactly one block in the cache
- lots of lower level blocks must share blocks in the cache
Address mapping 
(Block address) modulo (# of blocks in the cache)
Have a tag associated with each cache block that contains the address information(the upper portion
of the address) required to identify the block

A direct-mapped cache with 64 blocks, 16 bytes/block
(Assuming 32-bit address)
To what block number does address 1200 map?
- Block address = 1200/16 = 75
- Block number = 75 modulo 64 = 11

Associative Caches
- In a direct mapped cache a memory block maps to exactly one cache block
- Allow more flexible block placement

2. Fully associative cache (완전 연관)
- Allow given block to go in any cache entry
- Requires all entries to be searched at once
- Comparator per entry(expensive)

3. n-way set associative cache (집합 연관)
- Each set contains n entries
- Block number determines which set
(Block number) modulo (#Sets in cache)
- Search all entries in a given set at once
- n comparators(less expensive)

# Replacement Policy
Direct mapped : no choice
Set associative : 
- Prefer non-valid entry, if there is one
- Otherwhise, choose among entries in the set
Least-recently used(LRU) :
- the block replaced is the one that has been unused for the longest time
- Simple for 2-way, manageable for 4-way, too hard beyond that
->
1. Must have hardware to keep track of when each way's block was used relative to the other
blocks in the set
2. For 2-way set associative, takes one bit per set(set the bit when a block is referenced
and reset the other way's bit)
Random :
- Gives approximately the same performance as LRU for high associativity

Benefits of Set Associative Caches
- The choice of direct mapped or set associative depends on the cost of a miss versus the cost of
implementation
- Largest gains are in going from direct mapped to 2-way(20% + reduction in miss rate)

# Block size Considerations
Larger blocks should reduce miss rate
- Due to spatial locality
But in a fixed-sized cache
- Larger blocks -> fewer of them (block개수 줄어듦)
(More competition , increased miss rate)
- Larger blocks -> pollution
Larger miss penalty
- Can override benefit of reduced miss rate
- Early restart and critical-word-first can help

Handling Cache Hits
Read Hits(l$ and D$)
- this is what we want
Write hits(D$ only)
- require the the cache and memory to be consistent
(always write the date into both the cache block and the next level in the memory hierarchy
(write-through))
(writes run at the speed of the next level in the memory hierarchy (so slow) or can use a write
buffer and stall only if the write buffer is full)
- allow cache and memory to be inconsistent
(write the data only into the cache block (write-back the cache block to the next level in the 
memory hierarchy when that cache block is "evicted"(쫓겨남)))
(need a dirty bit for each data cache block to tell if it needs to be written back to memory
when it is evicted (can use a write buffer to help "buffer" write) backs of dirty blocks)

Handling Cache Misses



















