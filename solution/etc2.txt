Memory Hierarchy란?

컴퓨터 하드웨어의 역사적 발전에 관한 보고서를 작성하고 있는 학생이라고 가정하자. 서가에서 많은 책들을 
뽑아 와서 도서관의 책상에 앉아 자료 조사를 하는 중이다. 그 책들에서 당신이 쓰려고 하는 여러 중요한 컴
퓨터에 대한 내용을 찾았지만, EDSAC에 대한 내용은 없다는 것을 알았다. 당신은 서가로 다시 가서 다른 책
을 더 찾아야 한다. EDSAC에 대한 내용이 담겨 있는 초창기 영국 컴퓨터에 대한 책을 찾게 된다. 일단 책상
위에 관련된 많은 책들을 모아 놓으면, 그 안에서 당신이 필요로 하는 주제를 찾게 될 확률이 매우 높아지게
된다. 이렇게 되면 서가에 다시 갈 필요 없이 단지 책상 위의 책들만을 사용하여 많은 작업을 할 수 있게 될
것이다. 책상 위에 여러 권의 책을 놓아두면, 책상에 책을 한 권만 올려놓고 다른 책을 찾을 때마다 가져다
두고 새 책을 찾아오는 것과 비교할 때 많은 시간을 절약할 수 있게 된다.
도서관에 있는 책을 찾아볼 확률이 모두 똑같지 않듯이, 프로그램이 코드와 데이터를 접근할 확률이 모두 같지는
않다. 반면에 도서관에 있는 모든 책을 책상 위에 올려놓고는 우리가 필요로 하는 것을 빠르게 찾을 수 없듯이
많은 정보량을 갖는 큰 메모리에서 메모리 접근을 빠르게 하는 것은 불가능하다. 도서관에서 책을 찾는 방법과
프로그램이 동작하는 방법에는 이러한 지역성의 원칙(principle of locality)이 똑같이 적용된다.

The Memory Hierarchy Goal
Fact : Large memories are slow and fast memories are small
How do we create a memory that gives the illusion of being large, cheap and fast(most of the time)?
- with hierarchy
- with parallelism

The Principle of Locality
Program access a relatively small portion of the address space at any instant of time
A principle that makes having a memory hierarchy a good idea

If an item is referenced,
# Temporal locality(locality in time) : it will tend to be referenced again soon
-> keep most recently accessed data items closer to the processor
# Spatial locality : nearby items will tend to be referenced soon
-> move blocks consisting of contiguous(연속적) words closer to the processor

메모리 계층 구조는 서로 다른 속도와 크기를 갖는 여러 계층의 메모리로 구성되어 있다. 가장 빠른 메모리
는 더 느린 메모리보다 비트당 가격이 비싸기 때문에 대개 그 크기가 작다. 메모리 계층구조의 목적은 사용자
에게 가장 빠른 메모리가 갖고 있는 접근속도를 제공하면서 동시에 가장 싼 기술로 최대한 큰 메모리 용량을
제공하는 것이다. 프로세서에 가까운 계층은 먼 계층의 부분집합이고, 가장 낮은 계층에는 모든 데이터가 다
저장되어 있다. 캐시의 주요 목적은 더 느린 기본 스토리지 계층에 액세스해야 하는 필요를 줄임으로써 데이터 검색 성능을 높이는 것

# Terminology
Block(or line) : the minimum unit of information that is present(or not) in a cache
Hit Rate : the fraction of memory accesses found in a level of the memory hierarchy
Hit Time : Time to access that level which consists of 
Time to access the block + Time to determine hit/miss
Miss Rate : the fraction of memory accesses not found in a level of the memory hierarchy(1- (Hit Rate))
Miss Penalty : Time to replace a block in that level with the corresponding block from a lower
level which consists of
Time to access the block in the lower level + Time to transmit that block to the level that experienced
the miss + Time to insert the blok in that level + Time to pass the block to the requestor
Hit Time << Miss Penalty

How is the Hierarchy Managed?
registers and memory (by compiler (programmer))
cache and main memory (by the cache controller hardware)
main memory and disks
- by the operating system (virtual memory)
- virtual to physical address mapping assisted by the hardware(TLB)
- by the programmer (files)

# cache mapping 

1. Direct mapping(직접 사상)
메모리 주소에 기반을 두고 할당하는 것

Each memory block is mapped to exactly one block in the cache
- lots of lower level blocks must share blocks in the cache
Address mapping 
(Block address) modulo (# of blocks in the cache)
Have a tag associated with each cache block that contains the address information(the upper portion
of the address) required to identify the block

A direct-mapped cache with 64 blocks, 16 bytes/block
(Assuming 32-bit address)
To what block number does address 1200 map?
- Block address = 1200/16 = 75
- Block number = 75 modulo 64 = 11

Associative Caches
- In a direct mapped cache a memory block maps to exactly one cache block
- Allow more flexible block placement

2. Fully associative cache (완전 연관)
- Allow given block to go in any cache entry
- Requires all entries to be searched at once
- Comparator per entry(expensive)

3. n-way set associative cache (집합 연관)
- Each set contains n entries
- Block number determines which set
(Block number) modulo (#Sets in cache)
- Search all entries in a given set at once
- n comparators(less expensive)

# Replacement Policy
Direct mapped : no choice
Set associative : 
- Prefer non-valid entry, if there is one
- Otherwhise, choose among entries in the set
Least-recently used(LRU) :
- the block replaced is the one that has been unused for the longest time
- Simple for 2-way, manageable for 4-way, too hard beyond that
->
1. Must have hardware to keep track of when each way's block was used relative to the other
blocks in the set
2. For 2-way set associative, takes one bit per set(set the bit when a block is referenced
and reset the other way's bit)
Random :
- Gives approximately the same performance as LRU for high associativity

Benefits of Set Associative Caches
- The choice of direct mapped or set associative depends on the cost of a miss versus the cost of
implementation
- Largest gains are in going from direct mapped to 2-way(20% + reduction in miss rate)

# Block size Considerations
Larger blocks should reduce miss rate
- Due to spatial locality
But in a fixed-sized cache
- Larger blocks -> fewer of them (block개수 줄어듦)
(More competition , increased miss rate)
- Larger blocks -> pollution
Larger miss penalty
- Can override benefit of reduced miss rate
- Early restart and critical-word-first can help

# Handling Cache Hits
Read Hits(l$ and D$)
- this is what we want
Write hits(D$ only)
- require the the cache and memory to be consistent
(always write the date into both the cache block and the next level in the memory hierarchy
(write-through))
(writes run at the speed of the next level in the memory hierarchy (so slow) or can use a write
buffer and stall only if the write buffer is full)
- allow cache and memory to be inconsistent
(write the data only into the cache block (write-back the cache block to the next level in the 
memory hierarchy when that cache block is "evicted"(쫓겨남)))
(need a dirty bit for each data cache block to tell if it needs to be written back to memory
when it is evicted (can use a write buffer to help "buffer" write) backs of dirty blocks)

# Handling Cache Misses(데이터가 캐시에 없어서 충족시킬 수 없는 캐시 데이터 요청)

control unit은 실패를 탐지해야 하며, 메모리(또는 하위 수준의 캐시)로부터 데이터를 가져와서 실패를 처리해야 한다. 만약 캐시 내에 원하는 데이터가 존재하면 컴퓨터는 아무 일도 없는 것처럼 데이터를 사
용할 수 있다. cache hit을 처리할 수 있게 processor의 control unit을 수정하는 것은 쉬운 일이지만, 
실패의 경우 몇가지 조작이 더 필요하다. handling cache misses는 processor의 control unit과 별도
제어기의 공동 작업으로 처리된다. 

1. Handling Cache Misses(Single Word Block)
Read misses(l$ and D$)
- stall the pipeline, fetch the block from the next level in the memory hierarchy, install it
in the cache and send the requested word to the processor, then let the pipeline resume
Write misses(D$ only)
- stall the pipeline, fetch the block from the next level in the memory hierarchy, install it
in the cache( which may involve having to evict a dirty block if using a write-back cache),
write the word from the processor to the cache, then let the pipeline resume
- Write allocate - just write the word into the cache updating both the tag and data, no need
to check for cache hit, no need to stall
- No-write allocate - skip the cache write (but must invalidate that cache block since it will
 now hold stale data) and just write the word to the write buffer(and eventually to the next
 memory level), no need to stall if the write buffer isn't full

2. Multiword Block Considerations
Read misses(l$ and D$)
Processed the same as for single word blocks - a miss returns the entire block from memory
Miss penalty grows as block size grows
- Early restart - processor resumes execution as soon as the requested word of the block is
returned
- Requested word first - requested word is transferred from the memory to the cache(and 
processor) first
Nonblocking cache - allows the processor to continue to access the cache while the cache is
handling an earlier miss

Write misses(D$)
If using write allocate must first fetch the block from memory and then write the word to the
block(e.g. for 4 word blocks, a new tag, one word of data from the new block, and three words
of data from the old block)

# Average Access Time
Hit time is also important for performance
- A larger cache will have a longer access time. An increase in hit time will likely add 
another stage to the pipeline.
- At some point the increase in hit time for a larger cache will nullify(무효화) the 
improvement in hit rate leading to a decrease in performance.

Average memory access time (AMAT)
- AMAT = Hit time + Miss rate * Miss penalty

Example
cpu with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, l-cache miss rate = 5%
AMAT = 1 + 0.05 * 20 = 2ns (2 cycles per instruction)

다단계 캐시를 이용한 실패 손실 줄이기
현대의 모든 컴퓨터는 캐시를 사용한다. 프로세서의 빠른 클럭속도와 상대적으로 점점 느려지는 DRAM
접근시간의 차이를 더욱 줄이기 위해, 고성능 마이크로프로세서들은 캐시를 한 계층 더 사용한다. 이 
2차 캐시(secondary cache)는 주로 마이크로프로세서에 내장되어 있으며, 1차 캐시(primary cache)에서
실패가 발생할 때 접근된다. 2차 캐시가 원하는 데이터를 갖고 있으면 실패 손실은 2차 캐시의 접근 
시간이 되며, 이 값은 메인 메모리의 접근 시간보다 훨씬 작다. 1차 캐시와 2차 캐시 둘 다 데이터를 갖
고 있지 않은 경우에는 메인 메모리 접근이 필요하게 되고 더 큰 실패 손실이 발생하게 된다.

# Multilevel Caches
Primary cache attached to CPU
- small, but fast
Level-2 cache services misses from primary cache
- Larger, slower, but still faster than main memory
Main memory services L-2 cache misses
Some high-end systems include L-3 cache

Multilevel Cache Example
Given
- CPU base CPI = 1, clock rate = 4GHz
- Miss rate/instruction = 2%
- Main memory access time = 100ns
With just primary cache
- Miss penalty = 100ns/0.25ns = 400cycles
- Effective CPI = 1 + 0.02 * 400 = 9
Now add L-2 cache
- Access time = 5ns
- Global miss rate to main memory = 0.5%
New Performance with L-2 cache
- Primary miss with L-2 hit
(Penalty = 5ns/0.25ns = 20cycles)
- Primary miss with L-2 miss
(Extra penalty = 400cycles)
- CPI = 1 + 0.02 * 20 + 0.005 * 400 = 3.4
- Performance ratio = 9/3.4 = 2.6

# Multilevel Cache Considerations
Primary cache
- Focus on minimal hit time in support of a shorter clock cycle
(Smaller with smaller block sizes)
L-2 cache
- Focus on low miss rate to avoid main memory access to reduce the penalty of long
main memory access times
(Larger block sizes -> spatial locality, Higher levels of associativity -> low miss rate)
- Hit time has less overall impact
Results
- L-1 cache usually smaller than a single cache
- L-1 block size smaller than L-2 block size

The L1 cache is usually separated into separate Data and Instruction caches
The L2 and higher caches are normally unified caches, i.e. both instructions and data are
in the same cache

# Virtual Memory
Use main memory as a "cache" for secondary memory
- Allows efficient and safe sharing of memory among multiple programs
- Provides the ability to easily run programs larger than the size of physical memory
- Simplifies loading a program for execution by providing for code relocation(재배치)
(i.e. the code can be loaded anywhere in main memory) 
What makes it work? (the Principle of Locality)
- A program is likely to access a relatively small portion of its address space during any period of time
Each program is compiled into its own address space - a "virtual" address space
- During run-time each virtual address must be translated to a physical address(an address in main memory)


 














